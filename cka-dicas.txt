#CKA Exam 1.20
####
#For Windows: Ctrl+Insert to copy and Shift+Insert to #paste
#In addition, you might find it helpful to use the #Notepad (see top menu under 'Exam Controls') to manipulate text before pasting to the #command line.
#### 

####
#vi Find&Replace
#:%s/foo/bar/g
####

####
#Netcat -- Debug DNS & Network
#kubectl run nginx --image=nginx --restart=Never --port=80
#kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml
#kubectl exec -i -t dnsutils -- nc -z -v -w 2 <service-name> <port>
#kubectl exec -i -t dnsutils -- nslookup <service-name.namespace.svc> | <10-10-10-10.namespace.pod>
#kubectl exec -i -t dnsutils -- wget --spider 
#kubectl get pods -l <key=value> -o yaml | grep podIP
#kubectl get pods | grep -i podip
$podIP: 10.244.3.5
#Find Pod IP
#kubectl run curl --image=radial/busyboxplus:curl -i --tty
$curl -k https://10.244.3.5
####

#### Various POD Example ####
#POD Command Sleep
#kubectl run busybox --image=busybox --restart=Never --/bin/sh -c "sleep 3600"
#kubectl --namespace=NAME_SPACE run --restart=Never busybox --image=busybox -- sleep 1d

### Kubectl run with lots of options
#kubectl run busybox --image=busybox --requests='cpu=100m,memory=256Mi' --restart=Never --serviceaccount=admin --record=true --save-config=true --labels='key:value,app:busybox,tier=frontend'--env="key=value" -o yaml --dry-run=client -- /bin/sh -c "sleep 3600" > busybox.yaml

####
#POD runs command "env" save output to a file
#Kubectl run busybox --image=busybox --restart=Never --rm -it -- env >>envpod.txt
##--rm=false: If true, delete resources created in this command for attached containers.
####
#POD Says "Hello world" and exists
#kubectl run busyboxy --image=busybox -it --rm --restart=Never --/bin/sh -c 'echo hello world'
#kubectl get pods --> No pod busybox running

####
#POD with port
#kubectl run nginx --image=nginx:1.17.4 --restart=Never --port=80 --labels=app=nginx

####
#Find Pods first to be terminated
#k -n <namespace> describe pod | less -p Requests # describe all pods and highlight Requests
# OR --> empty results
#k -n project-c13 get pod \
  -o jsonpath="{range .items[*]} {.metadata.name}{.spec.containers[*].resources}{'\n'}"
# OR --> BestEffor QoSClass
#k get pods -n project-c13 \
  -o jsonpath="{range .items[*]}{.metadata.name} {.status.qosClass}{'\n'}"
​
####

####
#Ingress & Ingress Controller
#Examples:
#1. Create an ingress object named myingress with the following specification:
## Manages the host myingress.mydomain
## Traffic to the base path / will be forwarded to a service called main on port 80
## Traffic to the path /api will be forwarded to a service called api on port 8080
#
#kubectl create ingress myingress --rule="myingress.mydomain/=main:80" --rule="myingress.mydomain/api=api:8080"
#Apply the following YAML:
#
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myingress
spec:
  rules:
  - host: myingress.mydomain
    http:
      paths:
      - backend:
          serviceName: main
          servicePort: 80
        path: /
        pathType: Exact
      - backend:
          serviceName: api
          servicePort: 8080
        path: /api
        pathType: Exact

#### 01 ####
#ServiceAccount,ClusterRole,Rolebinding
##Create Namespace
#kubectl create ns <name>
##Create ClusterRole
#kubectl create clusterrole <clusterrole-name> --verb=create,delete,list,update,watch --resource=deployments,daemonsets,statefulsets --resource=pods,configmaps,secrets,namespaces --resource=pv --resource=pvc --namespace=processor $
do > clusterrole1.yaml
##Create RoleBinding (pay attention to exercise : Not ClusterRoleBinding)
#kubectl create rolebinding <role-binding-name> --clusterrole=<clusterrole-name> --serviceaccount=<namespace>:<serviceaccount-name> --namespace=<namespace-name> $do > rolebinding-processor.yaml
##Verify
#kubectl auth can-i create deploy -n processor --as system:serviceaccount:processor:processor
#kubectl auth can-i [create,delete,list,update,watch] [deploy,pod,namespace,pv,pvc...] -n <namespace-name> --as <system:serviceaccount:<namespace>:<serviceaccount-name>
$yes
####  

####
#JSONPATH
#kubectl get deployments.apps -o jsonpath='{.metadata.name}{.spec.template.spec.containers[].image}{.items.spec.replicas}{.metadata.namespace} --sort-by=.metadata.name
#kubectl -n admin2406 get deployment -o custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers[].image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace --sort-by=.metadata.name
# DEPLOYMENT:.metadata.name
# CONTAINER_IMAGE:.spec.template.spec.containers[].image
# READY_REPLICAS:.status.readyReplicas
# NAMESPACE:.metadata.namespace
# Creation: .metadata.creationTimestamp
#### 03 ####
###CUSTOM-COLUMNS
#kubectl get deployments.apps -o custom-columns='DEPLOYMENT:.metadata.name, CONTAINER_IMAGE:.spec.template.spec.containers[].image, READY_REPLICAS:.spec.replicas, NAMESPACE:.metadata.namespace' --sort-by=.metadata.name
#Custom Columns POD_NAME & POD_STATUS
#kubectl get pods -o custom-columns="POD_NAME:.metadata.name, POD_STATUS:.status.containerStatuses[].state"
# Save to a file
####
#### 03 ####
#Jsonpath sort pods created timestamp
#List all the pods sorted by created timestamp
#kubectl get pods -A --sort-by=.metadata.creationTimestamp
####
#Jsonpath List Pods name and namespace
#List all the pods showing name and namespace with a json path expression
#kubectl get pods -A -o=jsonpath="{.items[*]['metadata.name','metadata.namespace']}"
#kubectl get pods -A -o=jsonpath="{.items[*]['metadata.name','metadata.namespace']}"
####
#Jsonpath POD Name and Start time
#kubectl get pods -o jsonpath='{range.items[*]}{.metadata.name}{"\t"}{.status.podIP}{"\n"}{end}'
#
####

#### 04 ####
#POD Nodeselector
# Node Selector: disk=ssd
###vi pod-nodeselector.yaml
apiVersion: v1
kind: Pod
metadata: 
  name: nginx			#<-- Change
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:			#Add
    disk: ssd			#Add	
####

####
#Sort pod by CPU utilization
#kubectl top pod -l name=cpu-utilizer
#vi /opt/cpu-utilizer.txr
<add name of the pod with higher cpu>
####

####
#Add Annotate to a POD/Deploy/DaemonSet
#kubectl annotate pod <pod_name> name=webapp
#kubectl describe pod <pod_name> | grep -i annotations
##
#Update all pods in the namespace
#kubectl annotate pods --all description='my frontend running nginx'
####

####
#StaticPod on node labeled (Kubelet)
#Node=node-1
#sudo -i
#vi /var/lib/kubelet/config.yaml
#check entry: staticPodPath: /etc/kubernetes/manifests
#cd /etc/kubernetes/manifests
#
###vi static-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webtool
spec:
  containers:
  - name: nginx
    image: nginx
##End of File
#systemctl restart kubelet
#systemctl enable kubelet
#systemctl status kubelet
#kubectl get pod | grep webtool #<-- check if there is -node_name
#exit
####

####
#Deploy with 5 replicas
#kubectl create deploy deploy5 --image=nginx $do >> /tmp/spec_deployment.yaml
#vi /tmp/spec_deployment.yaml
###vi deploy-replicas.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app_runtime_stage: dev				#Change
  name: safari						#Cahnge
  namespace: project-tiger				#Change
spec:
  replicas: 5						#change
  selector:
    matchLabels:
      app_runtime_stage: dev				#Change
  strategy: {}
  template:
    metadata:
      labels:
        app_runtime_stage: dev				#Change
    spec:						
      containers:
      - image: nginx
        name: nginx
##End of File 
      
####
#Kubectl Logs 
#Extract logs from Pods
#kubectl logs <pod_name> | grep <specific_word>
#Word example: Extract log lines corresponding to error unable-to-access-website
# Kubectl logs <pod_name> | grep unable-to-access-website >> /opt/foo
#Kubectl Logs -- write logs to file
#kubectl logs frontend | grep -i "started" > /opt/error-logs
##
#Check logs from multi-pod 
#kubectl logs <pod_name> -c <container-name>
####

####
#List PV by storage capacity
#kubectl get pv --sort-by=.spec.capacity.storage >> /opt/volume_list
####

####
#Create a Pod Single App Container
#1 to 4 images
#Nginx + Redis + Mongo
###vi single-app-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: single-app
  name: single-app #<-- Change here
spec:
  containers:
  - image: nginx
    name: nginx
  - image: redis
    name: redis
  - image: consul
    name: consul
  - image: mongo
    name: mongo
##End of File 
####

####
#Service & PortNames
###vi pod-and-service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: voting-svc
  namespace: emojivoto
spec:
  ports:
  - name: grpc
    port: 8080
    targetPort: 8080
  - name: prom
    port: 8801
    targetPort: 8801
  selector:
    app: voting-svc
---
apiVersion: v1
kind: Pod
metadata:
  labels: 
    app: voting-svc
  name: my-pod
  namespace: my-namespace
spec:
  containers:
  - env:
    - name: HTTP_PORT
      value: "80"
    - name: ADMIN_PORT
      value: "8080"
    image: nginx
    name: nginx
    ports:
    - containerPort: 80
      name: http
    - containerPort: 8080
      name: admin
    resources:
      requests:
        cpu: 500m
        memory: 256Mi
  #serviceAccountName: admin
##End of File
#--- Deployment example ---#
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: voting
    app.kubernetes.io/part-of: emojivoto
    app.kubernetes.io/version: v11
  name: voting
  namespace: emojivoto
spec:
  replicas: 1
  selector:
    matchLabels:
      app: voting-svc
      version: v11
  template:
    metadata:
      labels:
        app: voting-svc
        version: v11
    spec:
      containers:
      - env:
        - name: GRPC_PORT
          value: "8080"
        - name: PROM_PORT
          value: "8801"
        image: docker.l5d.io/buoyantio/emojivoto-voting-svc:v11
        name: voting-svc
        ports:
        - containerPort: 8080
          name: grpc
        - containerPort: 8801
          name: prom
        resources:
          requests:
            cpu: 100m
      serviceAccountName: voting
####

####
#CERTIFICATES
#Get base64
#cat john.csr | base64 | tr -d "\n"
#
#VALIDATE_CERTIFICATE
#openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout
#
####
#KUBELET Certificates
#https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet
#/etc/systemd/system/kubelet.service.d/10-kubeadm.conf
#/var/lib/kubelet/pki
#Client certificate = /var/lib/kubelet/pki/kubelet-client-current.pem
#Find Issuer
#openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep Issuer
#Find Extended Key Usage:
#openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep "Extended Key Usage" -A1
#
#Server certificate = /var/lib/kubelet/pki/kubelet.crt
#Find Issuer:
#openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet.crt | grep Issuer
#Find Extended Key Usage:
#openssl x509  -noout -text -in /var/lib/kubelet/pki/kubelet.crt | grep "Extended Key Usage" -A1
####

####
#JOBS
###vi job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: sleepy
spec:
  template:
    spec:
      containers:
      - name: resting
        image: busybox
        command: ["/bin/sleep"]
        args: ["3"]
      restartPolicy: Never
##End of File
####

#######
#LivenessProbe and ReadinessProbe
#######
#containers liveprobness and readinessprobe:
##vi pod-liveness-and-readiness-probe.yaml
apiVersion: v1
kind: Pod
metadata:
  labels: 
    app: voting-svc
  name: my-pod
  namespace: my-namespace
spec:
  containers:
  - image: nginx:1.16.1-alpine
    name: ready-if-service-ready
    livenessProbe:                               # add from here
      exec:
        command:
        - 'true'
    readinessProbe:
      exec:
        command:
        - sh
        - -c
        - 'wget -T2 -O- http://service-am-i-ready:80'   # to here
##End of File

####
#Multi-POD Command & Variable & EmptyDir Volume:
#
##vi pod-cmd-env-emptydir.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: multi-container-playground
  name: multi-container-playground
spec:
  containers:
  - image: nginx:1.17.6-alpine
    name: c1                                                                      # change
    resources: {}
    env:                                                                          # add
    - name: MY_NODE_NAME                                                          # add
      valueFrom:                                                                  # add
        fieldRef:                                                                 # add
          fieldPath: spec.nodeName                                                # add
    volumeMounts:                                                                 # add
    - name: vol                                                                   # add
      mountPath: /vol                                                             # add
  - image: busybox:1.31.1                                                         # add
    name: c2                                                                      # add
    command: ["sh", "-c", "while true; do date >> /vol/date.log; sleep 1; done"]  # add
    volumeMounts:                                                                 # add
    - name: vol                                                                   # add
      mountPath: /vol
# Challenge: Create a Pod with 2 containers and Add 3rd container later,
# use description below
  - image: busybox:1.31.1                                                         # add
    name: c3                                                                      # add
    command: ["sh", "-c", "tail -f /vol/date.log"]                                # add
    volumeMounts:                                                                 # add
    - name: vol                                                                   # add
      mountPath: /vol                                                             # add
  volumes:                                                                        # add
    - name: vol                                                                   # add
      emptyDir: {}
##End of File

##Verify
#k exec -i -t multi-container-playground -c c1 -- printenv | grep -i MY_NODE_NAME
#kubectl logs multi-container-playground -c c1
#kubectl logs multi-container-playground -c c2
#kubectl logs multi-container-playground -c c3 #should see some logs here!#

####
# POD with Env & Secrets
#
#kubectl create secret generic super-secrect --from-literal=username=alice --from-literal=password='alice'
-------------------------------
##vi pod-secrect-env-via-file.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: secret-pod
  name: pod-secrect-via-file
  namespace: secret
spec:
  containers:
  - image: redis
    name: redis-secrets-via-file
    volumeMounts:
    - name: my-secrets
      mountPath: "/secrets"
      readOnly: true
  volumes:
  - name: my-secrets
    secret:
      secretName: super-secrect
##End of File
---------------------------
##vi pod-secrect-env-via-variable.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: secret-pod
  name: pod-secrect-via-env
  namespace: secret                       # add
spec:
  containers:
  - image: redis
    name: redis-secrets-via-env
    env:                                  # add
    - name: TOPSECRET                     # add
      valueFrom:                          # add
        secretKeyRef:                     # add
          name: super-secrect             # add
          key: username                   # add
##End of File
#Verify:
#kubectl exec -i -t pod-secrect-via-env -- printenv | grep -i TOPSECRET
#kubectl exec -i -t pod-secrect-via-file -- sh
$cd /secrets
$ls -l
$cd /secrets/..data
$ls -l
$-rw-r--r-- 1 root root 5 Mar  1 23:25 password
$-rw-r--r-- 1 root root 5 Mar  1 23:25 username
####

####
#POD with Master Taint&Toleration
##vi pod-taint-toleration.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "node-role.kubernetes.io/master"
    operator: "Exists"
    effect: "NoSchedule"
##End of File
####

####
#Get list of PV and PVC
#kubectl get pv
#Kubectl get pvc
#kubectl get pv,pvc
####

####
# PV Example
##vi pv-volume.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-volume		#<-- Change here
  namespace: safari
spec:
  capacity:
    storage: 10Mi
  accessModes:
    - ReadWriteMany		#<-- ReadWriteOnce
  hostPath:
    path: "/srv/app-data"
  storageClassName: shared	#<-- Optional... see if needed or not
##End of File
####
####
#PVC Example
##vi pod-pvc-volume.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-volume		#<-- Change here
  namespace: safari 		#<-- Change here
spec:
  accessModes:
    - ReadWriteMany		#<-- ReadWriteOnce
  resources:
    requests:
      storage: 5Mi
  storageClassName: shared	#<-- Optional... see if needed or not

#POD with PVC/MountPath
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pvc					#Cahnge
  namespace: safari					#Change
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: "/tmp/safari-data"		#<-- Check question requirement
      name: data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-volume			#<-- Same name of PVC
##End of File
####
####
#Deployment with PVC/MountPath
##vi deploy-pvc-volume.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: safari						#Change
  name: safari						#Cahnge
  namespace: project-tiger				#Change
spec:
  replicas: 1
  selector:
    matchLabels:
      app: safari					#Change
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: safari					#Change
    spec:						# From here to end can be a pod specification#
      containers:
      - image: httpd:2.4.41-alpine
        name: container
        volumeMounts:
        - mountPath: "/tmp/safari-data"		#<-- Check question requirement
          name: data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: pvc-volume			#<-- Same name of PVC
##End of File
####

####
#Undo deployment image version (previous / newer)
#kubectl rollout undo deploy <name_of_deploy> --record
#kubectl describe deploy <name_of_deploy> | grep -i Image
####

####
#List pods with different levels of verbosity
#kubectl run nginx --image=nginx --restart=Never --port=80
#kubectl get pod nginx --v=7
#kubectl get pod nginx --v=8
#kubectl get pod nginx --v=9
####

####
#Services Type NodePort
#kubectl run nginx --image=nginx --restart=Never --labels=app=my-nginx --port=80 $do >>nginx.pod.yaml
#kubectl expose pod nginx --name=<service_name> --type=NodePort --port=80 --namespace=<default> $do >nginx-pod-service.yaml
#vi nginx-pod-service.yaml
#Edit NodePort to 32767 | <port_number>
#kubectl create -f nginx.pod.yaml
#kubectl get pod | grep -i nginx
#kubectl create -f nginx-pod-service.yaml
#kubectl describe svc <service_name>
#check NodePort 
#kubectl get po,ep,svc # check all services, pods and endpoints
####

####
#POD Requests & Limits
#kubectl run nginx-request --image=nginx --restart=Always $do > nginx-request.yaml
##vi pod-request-limits.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: naginx-request
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      requests:
        memory: "100Mi"
        cpu: "0.5"
      limits:
        memory: "200Mi"
        cpu: "1"
##End of File
#kubectl apply -f nginx-request.yaml
#Verify:
#Kubectl top pod --name=nginx-request
####

####
# DaemonSet with request CPU/Memory
##vi ds-request-toleration.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    id: ds-important
    uuid: 18426a0b-5f59-4e10-923f-c0e078e82462
    app: ds-important
  name: ds-important #ds-kusc00201
spec:
  selector:
    matchLabels:
      app: ds-important
  template:
    metadata:
      labels:
        uuid: 18426a0b-5f59-4e10-923f-c0e078e82462
        id: ds-important
        app: ds-important
    spec:
      containers:
      - image: nginx
        name: nginx
        resources:					#Optional
          requests:					#Optional
            cpu: 10m					#Optional
            memory: 10Mi				#Optional
      tolerations:					#Toleration to Controlplane Nodes
      - key: "node-role.kubernetes.io/master"		#Taint from Master : Add node taints here
        operator: "Exists"				#Operator	
        effect: "NoSchedule"				#Effect
##End of File

##########
#Deployment with POD AntiAffinity
##vi deploy-pod-antiaffinity.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: deploy-important
    id: very-important
  name: deploy-important
  namespace: project-tiger
spec:
  replicas: 3
  selector:
    matchLabels:
      app: deploy-important
      id: very-important
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: deploy-important
        id: very-important
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: id
                operator: In
                values:
                - very-important
            topologyKey: "kubernetes.io/hostname"
      containers:
      - image: nginx:1.17.6-alpine
        name: container1
      - image: kubernetes/pause
        name: container2
##End of File
####


####
# POD & Service with Port name and containerPort
####
##vi pod-portname-and-service.yaml
apiVersion: v1
kind: Pod
metadata:
  name: named-port-pod
  labels:
    app: named-port-pod
spec:
  containers:
    - name: echoserver
      image: gcr.io/google_containers/echoserver:1.4
      ports:
      - name: pod-custom-port
        containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: named-port-svc
spec:
  ports:
    - port: 80
      targetPort: pod-custom-port
  selector:
    app: named-port-pod
##End of File

####

####
#Netpol - NetworkPolicy
#Allow traffic pods in all namespace
#with label "type=monitoring" to pods matching label "app:db"
#Check if namespace exist
#kubectl get ns
#kubectl create ns web
#kubectl label namespace/web app=web
##vi netpol-ingress-namespaceSelector.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-allow-all-ns-monitoring
  namespace: web
spec:
  podSelector:
    matchLabels:
      app: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          app: web
      podSelector:
        matchLabels:
          type: monitoring
####
#### Egress example ####
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np-backend
  namespace: project-snake
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Egress                    # policy is only about Egress
  egress:
    -                           # first rule
      to:                           # first condition "to"
      - podSelector:
          matchLabels:
            app: db1
      ports:                        # second condition "port"
      - protocol: TCP
        port: 1111
    -                           # second rule
      to:                           # first condition "to"
      - podSelector:
          matchLabels:
            app: db2
      ports:                        # second condition "port"
      - protocol: TCP
        port: 2222
####
#vi netpol-ingress-namespaceSelector.yaml
#kubectl create -f netpol-ingress-namespaceSelector.yaml

#Test you policy
kubectl run busybox --rm -ti --labels="type=monitoring" --image=busybox -- /bin/sh
#wget --spider --timeout=1 <pod_name>
#
#### Deny all ingress traffic ####
##vi netpol-ingress-deny-all.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
  namespace: web	#Optional
spec:
  podSelector: {}
  policyTypes:
  - Ingress
##End of File
#vi netpol-ingress-deny-all.yaml
#kubectl create -f deny-all-ingress-traffic.yaml
####

####
#Docker Logs
##
#ssh cluster1-worker2
#docker ps -a | grep -i <docker_name>
#identify docker ID
#docker logs <docker_id> &> /opt/course/17/pod-container.log
#####
#Aqui é o FIM!!!!!
EOF